---
title: "Модуль upmeter"
---

Модуль собирает статистику по типам доступности для компонентов кластера и Deckhouse. Позволяет оценивать степень выполнения SLA на эти компоненты, показывает данные о доступности в web-интерфейсе и предоставляет web-страницу статуса работы компонентов кластера.

С помощью Custom Resource [UpmeterRemoteWrite](cr.html#upmeterremotewrite) можно экспортировать метрики доступности по протоколу [Prometheus Remote Write](https://docs.sysdig.com/en/docs/installation/prometheus-remote-write/).

Состав модуля:
- **agent** — делает пробы доступности и отправляет результаты на сервер, работает на мастер-узлах.
- **upmeter** — агрегатор результатов и API-сервер для их извлечения.
- **front**
  - **status** — показывает текущий уровень доступности за последние 10 минут (по умолчанию требует авторизации, но её можно отключить).
  - **webui** — дашборд со статистикой по пробам и группам доступности (требует авторизации).
- **smoke-mini** — постоянное *smoke-тестирование* с помощью StatefulSet, похожего на настоящее приложение.

Модуль отправляет около 100 показаний метрик каждые 5 минут. Это значение зависит от количества включенных модулей Deckhouse.

## Интерфейс

Пример web-интерфейса:
![Пример web-интерфейса](../../images/500-upmeter/image1.png)

Пример графиков по метрикам из upmeter в Grafana:
![Пример графиков по метрикам из upmeter в Grafana](../../images/500-upmeter/image2.png)


## Концепция

### Термины

Агенты апметра `ds/upmeter-agent` собирают данные, запуская пробы. Каждые 30 секунд агенты отправляют на сервер `sts/upmeter` статистику доступности для проб и групп проб.

**Проверка** (check) — это способ измерения доступности конкретной функции приложения или компонента кластера.

**Проба** (probe) — это способ оценки доступности компонента кластера, компонента Декхауса или его функции. Проба состоит из одной или нескольких проверок.

**Группа доступности** (availablility group, group) — это группа проб, объединенная по принципу функциональности. Группа состоит из одной или более проб.  По группе оценивается доступность и соблюдение SLA согласно условиям соглашения.

Когда проверка завершается, она получает статус. Статус пробы вычисляется из статусов проверки, а статус группы — и зстатусов проб. Если за интервал измерения в пробе не прошла хотя бы одна проверка, то проба принимает соответствующий статус. Успешно прошедшие проверки игнорируются. Группа принимает статус самой неудачной пробы. Какие есть статусы:

* **Up** — проверка доступности положительная
* **Down** — проверка доступности отрицательная
* **Unknown** — данные собирались, но не смогли вынести вердикт доступности
* **Nodata** — данные не собирались, например если upmeter-agent не работал

Пример для статуса *Unknown*. Перед проверкой статуса пода (Ready или нет), делается проверка доступности аписервера — запрос на /version, как в пробе control-plane/apiserver. Если версию аписервера получить не удалось, проверка пода возвращает статус *Unknown*, потому что статус пода узнать невозможно.

### Алгоритмы вычисления доступности

Если проверки одной пробы завершились с разными статусами, то статус этой пробы вычисляется принципу наименьшего значения при отношении

    Down < Up < Unknown

Проба получит статус *Unknown*, если все проверки завершились с этим статусом. Если хотя бы одна проверка завершилась с *Up* и ни одной — с *Down*, то статус пробы будет *Up*. Если хотя бы одна проверка завершилась со статусом *Down*, то статус пробы будет *Down*.

Таким же способом складывается статус группы на основе статусов проб.

Агенты апметра отправляют статистику доступности за 30 секунд на сервер. Интервалы накоплений выбираются по системным часам кратно 30 секундам текущего времени. 30 секунд — это оперативный интервал. Статистика для групп вычисляется в агенте и отправляется на сервер совместно со статистикой для проб

Сервер комбинирует статистику от нескольких агентов по принципу самой удачной в интервал 5 минут. 5
минут — это интервал хранения данных и отправки метрик.

Как статус группы зависит от статуса проб:

  * Если хотя бы одна проба в группе имеет статус *Down*, то статус группы *Down*
  * Если нет статуса *Down* и есть хотя бы один статус *Up*, то статус группы *Up*
  * Если все пробы в группе имеют статус *Unknown*, то статус группы *Unknown*

### Сбор статистики статусов

Агенты апметра отправляют собирают статистику доступности за интервал накопления 30 секунд и отправляют ее на сервер. Чтобы собрать статистику, агент накапливает результаты проб в *массив статусов* для каждой пробы, и из этого массива собирает статистику.

Агент запускает проверки и записывает статусы проб в промежуточную таблицу. В этой таблице статус пробы обновляется с частотой запуска ее проверок. Интервал самой частой проверки — 200 мс. Поэтому каждые 200 мс агент собирает срез статусов с таблицы и добавляет их массивы статусов для каждой пробы. За 30 секунд будет 150 срезов статусов, это длина массива статусов у каждой пробы. По этим 150 статусам собирается статистика в единицах времени.

Пример массива статусов, где шаг в массиве 200 мс.

```
                       |------------ интервал накопления -----------|
запуск проверки        ↓________      ↓________      ↓________      |
статус проверки       o>________.______________X______________._____|
                                ↓              ↓              ↓
массив статусов       [ooooooooo...............XXXXXXXXXXXXXXX......]
N                          9          15              15         6

o — Nodata  = 9           ×200 мс = 1,8 c
. — Up      = 15 + 6 = 21 ×200 мс = 4,2 c
X — Down    = 15          ×200 мс = 3,0 c
u — Unknown = 0
```

Статус *Nodata*  возникает, когда агент запускается не точно в начале интервала накопления, поэтому
начальная часть интервала остается без данных.

Массив статусов группы вычисляется в агенте из массивов статусов проб. Возможно, что
две пробы будут иметь статус *Down* в разное время. Тогда статистика доступности у группы будет хуже, чем у каждой из проб. Пример такой ситуации:

```
             Массивы статусов   Доступность up/(up+down)
проба 1      ....XXXX.......    11/15 = 0.73
проба 2      ......XXXXX....    10/15 = 0.67
группа       ....XXXXXXX....     8/15 = 0.53
```

## Метрики



Севрер апметра генерирует метрику *status_time* (тип — *gauge*) — это время одного статуса за период 5 минут. Сервер копит статистику доступности за 5 минут и экспортирует время статусов раз в 5 минут. Статусы в метриках есть для индивидуальных проб и для групп целиком. Сумма статусов для пробы или группы — всегда пять минут. Единицы значений — миллисекунды. Севрер отправляет метрики в формате протокола [remote_write](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#remote_write).

Пример метрик для пробы и группы целиком:

```promql
# Пример для пробы
status_time{group="synthectic", probe="dns", status="up"}        # ≤ 300000

# Пример для группы
status_time{group="synthectic", probe="__total__", status="up"}  # ≤ 300000
```

> **Пример.** Возьмем для простоты только статусы *up* и *down*. В пяти минутах 300000 миллисекунд, в этих единицах приходит значение. Допустим, в точке метрики пришли значения с лейблами
> >
> ```
> 100000 status=down
> 200000 status=up
> ```
>
> Тогда
>
> ```
> availability = up/(down+up) = ⅔ = 66%
> ```

— это доступность группы или пробы за 5 минут.


Апметр оценивает время не-простоя с использоваением статуса *Unknown*: делит время «не-простоя» на
все измеренное время.

    availability = (up+unknown)/(down+up+unknown)


## Группы доступности и пробы

Бо̀льшая часть проверок доступности состоит из запросов к API-сервреру кластера. В этих проверках предварительно проверяется доступность API-сервера запросом на /version. Если апи-сервер недоступен, проверка завершается в статусе *Unknown*.

### Группа control-plane

#### apiserver

Проверяет доступность kube-apiserver. Делает запрос на /version.

Интревал: 5 секунд

#### basic-functionality

Проверяет работоспособность kube-apiserver на цикле жизни ConfigMap. Создает ConfigMap, удостоверятся в ее наличии, удаляет ConfigMap, проверяет, что ее нет.

Предварительно проверяет доступность аписервера. Если аписервер недоступен, статус пробы становится *Unknown*.

Интревал: 5 секунд

#### namespace

Проверяет работоспособность kube-apiserver на цикле жизни Namespace. Создает Namespace, удостоверятся в его наличии, удаляет Namespace, проверить что его нет.

Предварительно проверяет доступность аписервера. Если аписервер недоступен, статус пробы становится *Unknown*.

Интревал: 1 минута

#### controller-manager

Проверяет работосопосбность kube-controller-manager на жизненном цикле пода контроллера. Создает StatefulSet, проверяет что создался Pod (его состояние нам не важно, может оставаться в статусе Pending). Удаляет StatefulSet, проверяет Pod удалился.

Предварительно проверяет доступность аписервера. Если аписервер недоступен, статус пробы становится *Unknown*.

Интревал: 1 минута

#### scheduler

Проверяет работоспособность kube-scheduler. Создет Pod и проверяет заполенность поля `.spec.nodeName`.

Предварительно проверяет доступность аписервера. Если аписервер недоступен, статус пробы становится *Unknown*.

Интревал: 1 минута

### Группа deckhouse

#### cluster-configuration

Проверяет работу конфигурации кластера. Декхаус управляет конфигурацией за счет хуков, поэтому эта проба проверяет работу хука.

Для этой пробы существует CRD UpmeterHookProbe. Апметр создаает CR UpmeterHookProbe, если его нет, по одному на каждый Pod upmeter-agent.

В CR апметр перезаписывает поле `spec.initial` случайным значением и ждет, пока поле `spec.mirror` станет равным этому же значению. Хук Декхауса дублирует значение из этого поля в поле `spec.mirror`. Если значение синхронизируется за 30 секунд, проба принимает статус Up.

Период: 1 минута. Предварительно проверяет доступность аписервера. Если аписервер недоступен, статус пробы становится *Unknown*.

Предварительно проверяет статус пода Декхауса:

  - если статус пода Terminating, статус пробы становится *Unknown*,
  - если статус пода Running, но не Ready менее 20 минут, статус пробы становится *Unknown*,
  - если статус пода Running, но не Ready более 20 минут, статус пробы становится *Down*.

### Группа extensions

#### cluster-autoscaler

Проверяет, что хотя бы 1 под cluster-autoscaler в состоянии Ready

Интервал: 10 секунд

#### cluster-scaling

Состоит из трех независимых проверок. Проверяет, что хотя бы 1 под в состоянии Ready у деплойментов machine-controller-manager, cloud-controller-manager, bashible-apiserver

Интервал: 10 секунд. Предварительно проверяет доступность аписервера в каждой проверке.


#### grafana

Проверяет, что хотя бы 1 под Grafana в состоянии Ready.

Интервал: 10 секунд. Предварительно проверяет доступность аписервера.

#### openvpn

Проверяет, что хотя бы 1 под OpenVPN в состоянии Ready

Интервал: 10 секунд

#### prometheus-longterm

Состоит из двух проверок:

- хотя бы 1 под StatefulSet prometheus-longterm в состоянии Ready, предварительно проверяет доступность аписервера;
- API отвечает "1" через сервис на запрос /api/v1/query?query=vector(1)

Интервал обеих проверок: 10 секунд.

#### dashboard

Хотя бы 1 под Dashboard в состоянии Ready

Интервал: 10 секунд. Предварительно проверяет доступность аписервера.

#### dex

Состоит из двух проверок:

- хотя бы 1 под Dex в состоянии Ready, предварительно проверяет доступность аписервера;
- API отвечает через сервис на запрос /keys

Интервал обеих проверок: 10 секунд.

### Группа load-balancing

#### load-balancer-configuration

Хотя бы 1 под cloud-controller-manager в состоянии Ready

Интервал: 10 секунд. Предварительно проверяет доступность аписервера.


#### metallb

Состоит из двух проверок:

- хотя бы 1 под MetalLB Controller в состоянии Ready
- хотя бы 1 под MetalLB Speaker в состоянии Ready

Интервал обеих проверок: 10 секунд. Предварительно проверяет доступность аписервера в обеих проверках.

### Группа monitoring-and-autoscaling

#### prometheus

Состоит из двух проверок:

- хотя бы 1 под StatefulSet prometheus-main в состоянии Ready, предварительно проверяет доступность аписервера;
- API отвечает "1" через сервис на запрос /api/v1/query?query=vector(1)

Интервал обеих проверок: 10 секунд.

#### trickster

Состоит из двух проверок:

- хотя бы 1 под Deployment trickster в состоянии Ready, предварительно проверяет доступность аписервера;
- API отвечает "1" через сервис на запрос /trickster/main/api/v1/query?query=vector(1)

Интервал обеих проверок: 10 секунд.

#### prometheus-metrics-adapter

Состоит из двух проверок:

- хотя бы 1 под Deployment prometheus-metrics-adapter в состоянии Ready, предварительно проверяет доступность аписервера;
- API отвечает "1" через сервис на запрос /apis/custom.metrics.k8s.io/v1beta1/namespaces/d8-upmeter/metrics/memory_1m

Интервал обеих проверок: 5 секунд.

#### vertical-pod-autoscaler

Состоит из трех независимых проверок.  Проверяет, что хотя бы 1 под в состоянии Ready у деплойментов vpa-updater, vpa-recommender, vpa-admission-controller.

Интервал: 10 секунд. Предварительно проверяет доступность аписервера во всех трех проверках.

#### metric-sources


- Все поды node-exporter в состоянии Ready на всех нодах, где должны быть, предварительно проверяет доступность аписервера. Однако не учитываются ноды,
  - которым меньше 10 минут
  - которые в процессе удаления (deletionTimestamp)
  - Закордоненные

- хотя бы один под kube-state-metrics в состоянии Ready, предварительно проверяет доступность аписервера.

Интервал обеих проверок: 10 секунд.

#### key-metrics-present

- в prometheus есть метрики от kube-state-metrics
- в prometheus есть метрики от node-exporter
- в prometheus есть метрики от kubelet

Интервал всех проверок: 15 секунд.

#### Horizontal-pod-autoscaler (вычисляемая проба)

Это вычисляемая проба. Она не делает самостоятельных проверок, только комбинирует статусы других

- control-plane/controller-manager
- monitoring-and-autoscaling/prometheus-metrics-adapter

### Группа Nginx

Пробы создаются динамически для имеющихся контроллеров. Каждая проба называется по имени контроллера. Она проверяет, что хотя бы один под в состоянии Ready. Предварительно проверяет доступность аписервера.

Интервал: 5 секунд.

### Группа Node Groups

Пробы создаются динамически для нод-групп, у которых `NG.spec.CloudInstances.minPerZone` > 0. Каждая проба называется по имени нод-группы. Она проверяет, что количество нод в нод-группе соответствует ожидаемому в каждой зоне. Предварительно проверяет доступность аписервера.

Интервал: 10 секунд.

### Группа Synthetic

Группа нацелена отслеживать сетевую связность между узлами, покрывая все пары узлов со временем
в случайном порядке. Проверка проходит в сети подов.

Для этого запускается модельное приложение smoke-mini. Проверяется связность как между 5 подами
smoke-mini, так и с подами upmeter-agent, работающих на мастерах. Раз в минуту один из подов
smoke-mini переносится на другой узел. Максимум задействованы 8 узлов: 3 мастера и 5 не-мастеров.

Смок-мини не масштабируется с размером кластера. Поэтому для больших кластеров он не приносит
оперативную информацию о связности сети. Недоступность сети подов у малого количества узлов
в большом кластере имеет слабый эффект на статусы проб.

#### access

Проверяет, что хотя бы один pod smoke-mini отвечает HTTP-статусом 200. Список подов берется из DNS.

Интервал: 5 секунд.

#### dns

Две проверки:

- хотя бы один pod smoke-mini отвечает HTTP-статусом 200 на /dns. Он резолвит `kubernetes.default`
- upmeter-agent резолвит внутренний домен `kubernetes.default.svc.<<global.discovery.clusterDomain>>`

Интервал: 200 миллисекунд.

#### neighbour

Хотя бы один pod smoke-mini отвечает HTTP-статусом 200 на /neighbor. Список pod’ов берется из DNS.
smoke-mini опрашивает соседние поды по имени Headless сервиса.

Интервал: 5 секунд.

#### neighbour-via-service

Хотя бы один pod smoke-mini отвечает HTTP-статусом 200 на /neighbour-via-service. Список pod’ов берется из DNS.
Smoke-mini делает 4 запроса на общий сервис ClusterIP и отвечает 200, если было не больше 2 ошибок.

Интервал: 5 секунд.
